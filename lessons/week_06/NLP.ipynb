{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preface"
      ],
      "metadata": {
        "id": "OKDxADWfMW6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Natural Language Processing**, commonly abbreviated as **``NLP``**, involves the computational handling and interpretation of human language, be it in spoken or written form. This field, rooted in ``linguistics``, has evolved significantly over the past half-century, especially with the advent of computer technology. In this lesson, we will delve into the essence of natural language processing and its significance.\n",
        "\n",
        "When dealing with textual data in the realm of **computer engineering**, it's essential to master three core domains:\n",
        "\n",
        "- **Text Preprocessing**: This encompasses the tasks of loading, analyzing, filtering, and refining text data before any computational modeling.\n",
        "- **Text Representation**: Beyond the traditional bag-of-words model, it's crucial to understand the advanced distributed representations like word embeddings.\n",
        "- **Text Generation**: This domain covers a spectrum of intriguing challenges, from generating image captions to facilitating machine translation."
      ],
      "metadata": {
        "id": "CFS1FmqhL_r4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n"
      ],
      "metadata": {
        "id": "5zZllkN8kwhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transitioning from ``raw text`` to building a ``machine learning`` or ``deep learning`` model isn't a direct process.\n",
        "\n",
        "Initially, you need to **preprocess your text**, which involves tasks like breaking it into words, handling punctuation, and addressing letter casing.\n",
        "\n",
        "In fact, there's a range of text preparation techniques that may be necessary, and the specific methods you choose will depend on your natural language processing objectives. In this section, we will explore how to clean and preprocess text effectively to make it ready for machine learning modeling. By the end of this section, you will have learned:\n",
        "\n",
        "- The basics of creating your own simple text cleaning tools.\n",
        "- How to progress to using more advanced techniques available in the [NLTK library](https://www.nltk.org/).\n",
        "- Key considerations when preparing text for natural language processing models."
      ],
      "metadata": {
        "id": "DujomyVICOIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metamorphosis by Franz Kafka"
      ],
      "metadata": {
        "id": "K5i7jLH-C4lL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll begin by choosing a dataset.\n",
        "\n",
        "In this section, we'll be working with the text from the book **``Metamorphosis``** by [Franz Kafka](https://www.amazon.com.br/Metamorfose-Edi%C3%A7%C3%A3o-Exclusiva-Amazon/dp/6580210001/). There isn't a specific reason for this choice other than the fact that it's relatively short, I personally enjoy it, and you might too. I anticipate that it's one of those classic pieces of literature that many students encounter in school. The complete text of **``Metamorphosis``** is freely available from [Project Gutenberg](https://www.gutenberg.org/). You can download the ASCII text version of the book from the code below:"
      ],
      "metadata": {
        "id": "IDvvcop2DH7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL of the file you want to download\n",
        "url = \"http://www.gutenberg.org/cache/epub/5200/pg5200.txt\"\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Open a local file to save the response content\n",
        "    with open(\"pg5200.txt\", \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(\"Download completed successfully. The file has been saved as 'pg5200.txt'\")\n",
        "else:\n",
        "    print(f\"Error downloading the file. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "Simd-uVQk0OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename the file\n",
        "!mv pg5200.txt metamorphosis.txt"
      ],
      "metadata": {
        "id": "Mza6FehJ7JyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the file and save it to your current working directory as **``metamorphosis.txt``**. This file includes header and footer details, specifically copyright and license information, which we don't need.\n",
        "\n",
        "- Open the file, remove the header and footer sections. The beginning of the cleaned-up file should appear as:\n",
        "\n",
        "> One morning, when Gregor Samsa woke from troubled dreams, he found himself\n",
        "transformed in his bed into a horrible vermin.\n",
        "\n",
        "The file should end with:\n",
        "\n",
        "> And, as if in confirmation of their new dreams and good intentions, as soon as they reached their destination Grete was the first to get up and stretch out her young body."
      ],
      "metadata": {
        "id": "cpqCJXsv5zdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete lines 1 to 44\n",
        "!sed -i '1,44d' metamorphosis.txt"
      ],
      "metadata": {
        "id": "m7UGdh-17qPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete lines 1861 to 2225\n",
        "!sed -i '1861,2225d' metamorphosis.txt"
      ],
      "metadata": {
        "id": "Qr0vzrNy77zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load text\n",
        "filename = 'metamorphosis.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()"
      ],
      "metadata": {
        "id": "II_BVHE9Bgn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "WYPokMbcEmnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning is Task-Specific"
      ],
      "metadata": {
        "id": "gR1u7bvgEsnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've acquired your text data, the initial step in tidying it up is to have a clear objective in mind. In this light, assess your text to pinpoint what might be beneficial. Spend a moment examining the text. What stands out to you? From my perspective:\n",
        "\n",
        "- The content is in plain text, eliminating the need to process any markup.\n",
        "- The content has been translated from the original German and utilizes UK English conventions, such as \"travelling.\"\n",
        "- Text lines break roughly after every 70 characters.\n",
        "- I couldn't spot any blatant typos or spelling errors.\n",
        "- Various punctuation marks, like commas, apostrophes, quotation marks, and question marks, are present.\n",
        "- Hyphenated descriptors, such as \"armour-like,\" are used.\n",
        "- Em dashes are frequently employed to extend sentences—perhaps consider substituting with commas?\n",
        "- Names like \"Mr. Samsa\" are mentioned.\n",
        "\n",
        "\n",
        "If I may humbly observe, it seems there aren't any numerical values that might need attention, such as \"1999\". Additionally, one can notice section markers, for instance, \"II\" and \"III\". To someone with keen insight, there may be even more intricate details to discern. In this section, we'll be graciously guiding you through the foundational steps of text cleaning. While doing so, it would be beneficial to ponder upon the specific goals we might aim to achieve with this text document. For instance:\n",
        "\n",
        "Should we aspire to craft a Kafkaesque language model, it might be prudent to retain all the case, quotes, and accompanying punctuation.\n",
        "Conversely, if our ambition were to categorize documents into \"Kafka\" and \"Not Kafka\", perhaps it would be advantageous to eliminate case, omit punctuation, and even reduce words to their roots.\n",
        "I would kindly encourage using your task's objectives as a guiding principle when preparing your text data."
      ],
      "metadata": {
        "id": "24U_VP8479e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual Tokenization"
      ],
      "metadata": {
        "id": "o0RwZZIPXjz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refining text typically involves transforming it into a list of words or tokens suitable for our machine learning endeavors. Essentially, this entails transforming the original text into a sequence of words and storing it. A straightforward method to achieve this is by dividing the document based on white spaces, encompassing spaces, newline characters, tabs, and the like. In Python, this can be gracefully accomplished using the **``split()``** method on the loaded string."
      ],
      "metadata": {
        "id": "jC57w3U6ZEaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into words by white space\n",
        "words = text.split()\n",
        "print(words[:100])"
      ],
      "metadata": {
        "id": "SWpUqxBYYiO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the given sample breaks the document into an extensive list of words, displaying the first 100 for our perusal. It's pleasing to observe that punctuation remains intact, as seen in words like **``\"wasn't\"``** and **``\"armour-like\"``**. However, it's less ideal to note that the punctuation marking the end of sentences remains attached to the last word, as in **``\"thought.\"``**\n",
        ".\n",
        "```python\n",
        "['One', 'morning,', 'when', 'Gregor', 'Samsa' 'woke', 'from', 'troubled', 'dreams,', 'he',\n",
        "'found', 'himself', 'transformed', 'in', 'his' 'bed', 'into', 'a', 'horrible', 'vermin.', 'He',\n",
        "'lay', 'on', 'his', 'armour-like', 'back,', 'and' 'if', 'he', 'lifted', 'his', 'head', 'a', 'little',\n",
        "'he', 'could', 'see', 'his', 'brown', 'belly,' 'slightly', 'domed', 'and', 'divided', 'by',\n",
        "'arches', 'into', 'stiff', 'sections.', 'The' 'bedding', 'was', 'hardly', 'able', 'to', 'cover',\n",
        "'it', 'and', 'seemed', 'ready', 'to', 'slide' 'off', 'any', 'moment.', 'His', 'many', 'legs',\n",
        "'pitifully', 'thin', 'compared', 'with', 'the' 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved',\n",
        "'about', 'helplessly', 'as', 'he', 'looked.', 'What’s', 'happened', 'to', 'me?”', 'he',\n",
        "'thought', 'It', 'wasn’t', 'a', 'dream.', 'His', 'room,','a', 'proper', 'human']\n",
        "```"
      ],
      "metadata": {
        "id": "xd07sEn8Z0T1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another approach might be to use the [regex model (re)](https://docs.python.org/3/library/re.html) and split the document into words by\n",
        "selecting for strings of alphanumeric characters **``(a-z, A-Z, 0-9 and _)``**. For example:"
      ],
      "metadata": {
        "id": "Da5ReqP2bTaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# split based on words only\n",
        "words = re.split(r'\\W+', text)\n",
        "print(words[:100])"
      ],
      "metadata": {
        "id": "j8QFdebPYjME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself',\n",
        "'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like',\n",
        "'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly',\n",
        "'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was',\n",
        "'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His',\n",
        "'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved',\n",
        "'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It',\n",
        "'wasn', 't', 'a', 'dream', 'His', 'room']\n",
        "```"
      ],
      "metadata": {
        "id": "LULorquKWe8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon executing the sample once more, we obtain our desired list of words. Now, it's evident that **``\"armour-like\"``** has been split into two separate words: **``\"armour\"``** and **``\"like\"``** (which is satisfactory). However, contractions such as **``\"What's\"``** have also been divided into **``\"What\"``** and **``\"s\"``** (which isn't quite optimal)."
      ],
      "metadata": {
        "id": "vBVXWIRCYvJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might desire the words but without punctuations such as commas and quotes. Additionally, it's preferable to retain contractions as a single unit. One approach is to segment the document into words based on white space, followed by utilizing string translation to eliminate all punctuation. Python conveniently offers a constant named **``string.punctuation``** which encompasses a comprehensive set of punctuation symbols. For instance:"
      ],
      "metadata": {
        "id": "Csmc-ATudD1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# split into words by white space\n",
        "words = text.split()\n",
        "\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "# remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in words]\n",
        "print(stripped[:100])"
      ],
      "metadata": {
        "id": "xw4wJ33Wb-l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "id": "mKrygUPQYbrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "It's a typical practice to unify all words to a single case. While this action reduces the vocabulary's size, it can lead to the loss of certain nuances (a classic illustration being the distinction between 'Apple' the corporation and 'apple' the fruit). We can transform all words to lowercase by invoking the **lower()** method on each word. As an illustration:"
      ],
      "metadata": {
        "id": "1s-KF-Sydo9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into words by white space\n",
        "words = text.split()\n",
        "# convert to lower case\n",
        "words = [word.lower() for word in words]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "id": "C0IpHd4acrE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text cleaning can be challenging, is often tailored to specific problems, and comes with its compromises. It's essential to keep in mind that simplicity is key. Opt for more straightforward text data, streamlined models, and smaller vocabularies. Complexity can always be added later to determine if it enhances the model's performance. Up next, we'll explore tools within the [NLTK library](https://www.nltk.org/) that provide functionalities beyond basic string division."
      ],
      "metadata": {
        "id": "-L0RU6wbd1qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and Cleaning with NLTK"
      ],
      "metadata": {
        "id": "8zipLeHCeD61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Natural Language Toolkit, abbreviated as NLTK, is a Python library designed for text processing and modeling. It offers valuable tools for importing and preprocessing text, preparing it for use with machine learning and deep learning models.\n",
        "\n",
        "> You can download NLTK via your preferred package manager, like pip. For machines compatible with POSIX, the command would be:\n",
        "\n",
        "```\n",
        "sudo pip install -U nltk\n",
        "```"
      ],
      "metadata": {
        "id": "KHb2-YqRaDnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once installed, you'll need to set up the data associated with the library, which includes a comprehensive collection of documents useful for testing other NLTK tools. There are several methods to achieve this, one of which is through a script:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download()\n",
        "```\n",
        "\n",
        "Or from command line:"
      ],
      "metadata": {
        "id": "P8nshUadakpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m nltk.downloader all"
      ],
      "metadata": {
        "id": "XtPj-DBUeWIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "metadata": {
        "id": "rx8XK1JhaK9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Segmentation\n"
      ],
      "metadata": {
        "id": "PUTEq3Sba7fK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "An initial beneficial step is to break the text down into individual sentences. Some modeling techniques, like **``Word2Vec``**, often work better with inputs formatted as paragraphs or sentences.\n",
        "\n",
        "You can begin by segmenting your **text into sentences**, further dividing each sentence into words, and then saving each sentence as a separate line in a file. NLTK offers the **``sent_tokenize()``** function for this purpose. In the following example, we load the **``metamorphosis.txt``** file, segment it into sentences, and display the first sentence."
      ],
      "metadata": {
        "id": "W3JvaupRbSXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "\n",
        "# load data\n",
        "filename = 'metamorphosis.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "\n",
        "# split into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences[0])"
      ],
      "metadata": {
        "id": "pN8GoxRZlgh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split to words"
      ],
      "metadata": {
        "id": "2MYyKkVfbdzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK offers a function named **``word_tokenize()``** that divides **strings into tokens**, which are typically **words**. This function segments tokens considering white space and punctuation. As such, punctuation marks like commas and periods are treated as individual tokens.\n",
        "\n",
        "> Additionally, contractions are divided (for instance, \"What's\" is tokenized into \"What\" and \"'s\"). Quotation marks remain intact, among other features. Here's an example:"
      ],
      "metadata": {
        "id": "33k1a1Yibp8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:100])"
      ],
      "metadata": {
        "id": "knHK0fkusFDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the code reveals that punctuation marks are tokenized, giving us the option to filter them out if desired.\n",
        "\n",
        "\n",
        "```python\n",
        "['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he',\n",
        "'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.',\n",
        "'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a',\n",
        "'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and',\n",
        "'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly',\n",
        "'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.',\n",
        "'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the',\n",
        "'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '“',\n",
        "'What', '’', 's', 'happened']\n",
        "```"
      ],
      "metadata": {
        "id": "YLeeQl6BsPGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Punctuation\n",
        "\n"
      ],
      "metadata": {
        "id": "XqKmIU3ScL-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can exclude tokens that aren't of interest, including standalone punctuation marks. This can be achieved by iterating through each token and retaining only those that are entirely alphabetic. Python provides the **``isalpha()``** function for this purpose. Here's an example:"
      ],
      "metadata": {
        "id": "SfWKstJscy2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# remove all tokens that are not alphabetic\n",
        "words = [word for word in tokens if word.isalpha()]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "id": "QPmejMRRc1ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself',\n",
        " 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and',\n",
        "'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly',\n",
        "'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly',\n",
        "'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs',\n",
        "'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about',\n",
        "'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a',\n",
        " 'dream', 'His', 'room', 'a', 'proper']\n",
        "```"
      ],
      "metadata": {
        "id": "diBgHE6qeOGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminate Stop Words (and Integration Process)\n"
      ],
      "metadata": {
        "id": "b_XYCG21dGNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Stop words** are typically words that don't add significant meaning to a sentence. They include common words like \"the,\" \"a,\" and \"is.\" In tasks like document classification, it can be beneficial to remove these stop words. NLTK offers a set of universally recognized stop words for various languages, including English. Here's how you can load them:"
      ],
      "metadata": {
        "id": "nV2WMKjWdVeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "nz3s1BvfdbZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
        "'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
        "\"it's\",'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',\n",
        "\"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having',\n",
        "'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
        "'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below',\n",
        " 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
        "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
        "'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\",  \n",
        "'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'no',\n",
        "'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn',\n",
        "\"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
        " 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "```"
      ],
      "metadata": {
        "id": "2MWr6CF2gP2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you run this code, it will print out a list of language codes that correspond to the languages for which NLTK provides stop words."
      ],
      "metadata": {
        "id": "7Nh5jTNud0ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# List all languages supported by the stopwords corpus\n",
        "supported_languages = stopwords.fileids()\n",
        "\n",
        "print(supported_languages)"
      ],
      "metadata": {
        "id": "sE4nzjQBdgse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided stop words are all in lowercase and devoid of punctuation. When comparing your tokens to these stop words for filtering, it's crucial to preprocess your text consistently. Here's a brief guide on setting up a text preparation pipeline:\n",
        "\n",
        "- Load the raw text.\n",
        "- Tokenize the text.\n",
        "- Convert tokens to lowercase.\n",
        "- Strip each token of punctuation.\n",
        "- Retain only alphabetic tokens.\n",
        "- Exclude tokens identified as stop words."
      ],
      "metadata": {
        "id": "8NzMmHxsd13H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load data\n",
        "filename = 'metamorphosis.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# convert to lower case\n",
        "tokens = [w.lower() for w in tokens]\n",
        "\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "# remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in tokens]\n",
        "\n",
        "# remove remaining tokens that are not alphabetic\n",
        "words = [word for word in stripped if word.isalpha()]\n",
        "\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w for w in words if not w in stop_words]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "id": "42N2Fb8yeW8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Stemming Words\n"
      ],
      "metadata": {
        "id": "oUDmmLEqktri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Stemming is the process of truncating words to their fundamental form or base. For instance, **``fishing``**, **``fished``**, and **``fisher``** all stem to **``fish``**. In tasks like document classification, stemming can be advantageous as it not only condenses the vocabulary but also emphasizes the sentiment or general intent of a document over its intricate meaning. Various stemming techniques exist, with the **Porter Stemming** algorithm being one of the most renowned and enduring. This algorithm can be accessed in NLTK using the **``PorterStemmer``** class. Here's a demonstration:"
      ],
      "metadata": {
        "id": "OsN3PEntlD6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# load data\n",
        "filename = 'metamorphosis.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# stemming of words\n",
        "porter = PorterStemmer()\n",
        "\n",
        "stemmed = [porter.stem(word) for word in tokens]\n",
        "print(stemmed[:100])"
      ],
      "metadata": {
        "id": "c83PpFdUlVHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "['one', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself',\n",
        " 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'he', 'lay', 'on', 'hi', 'armour-lik',\n",
        "'back', ',', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he','could', 'see', 'hi', 'brown', 'belli',\n",
        "',', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', '.', 'the', 'bed', 'wa', 'hardli',\n",
        "'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', '.', 'hi', 'mani', 'leg',\n",
        "',', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'wave', 'about',\n",
        "'helplessli', 'as', 'he', 'look', '.', '“', 'what', '’', 's', 'happen']\n",
        "```"
      ],
      "metadata": {
        "id": "ErthEBufjE_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon executing the example, it's evident that words are truncated to their root forms, for instance, **``trouble``** transforms to **``troubl``**. Additionally, the stemming process also converts tokens to lowercase, presumably for efficient internal referencing in word tables."
      ],
      "metadata": {
        "id": "hw6SIjC2ld7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expanding on Text Cleaning: A Comprehensive Overview\n"
      ],
      "metadata": {
        "id": "DS4OvDN2lw6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the context of our preliminary discussion, it is pertinent to note that the source text utilized was relatively uncomplicated. However, when delving into real-world applications, several additional layers of complexity emerge in text cleaning. The following outlines some critical considerations:\n",
        "\n",
        "- **Large-Scale Documents**: How does one efficiently handle extensive documents or vast collections of textual data that exceed memory constraints?\n",
        "- **Structured Document Extraction**: There's a challenge in gleaning text from formats like HTML, PDF, or other structured document types.\n",
        "- **Transliteration Concerns**: The task of converting characters from non-Latin scripts to the English alphabet poses its unique set of problems.\n",
        "- **Unicode Decoding**: One must address the decoding of Unicode characters into a standardized form, such as UTF-8.\n",
        "- **Domain-Specific Lexical Units**: Specialized terminologies, acronyms, and phrases in certain fields may necessitate specialized treatment.\n",
        "Numerical Entities: Handling or omitting numerics, which can range from dates to quantities, requires careful consideration.\n",
        "- **Typographical Errors**: Identifying and rectifying common typographical errors and misspellings is a crucial step.\n",
        "- **Additional Factors**: The list is by no means exhaustive, and there are myriad other considerations based on specific project requirements.\n",
        "\n",
        "It's imperative to understand that achieving an entirely **``clean``** text, devoid of all inconsistencies, is a formidable challenge. The notion of **``cleanliness``** in text is often contextual, determined by the objectives and parameters of a specific project. It is recommended to consistently evaluate your data post each transformation phase. The practice of storing intermediary data post every transformation can be beneficial for in-depth analysis, as nuances and issues often become apparent upon closer inspection.\n",
        "\n",
        "The overarching principle, as emphasized in this discourse, is that effective text cleaning is a meticulous balance of resources, time, and domain knowledge."
      ],
      "metadata": {
        "id": "k7c-76aCmQQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Data Preparation Using Scikit-learn: A Guide"
      ],
      "metadata": {
        "id": "Yuhn2QPpmtK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Textual data necessitates unique pre-processing steps to make it suitable for predictive modeling.\n",
        "\n",
        "> Initially, it involves parsing the text to separate and identify individual words, a process known as **tokenization**.\n",
        "\n",
        "Subsequently, these words must be translated into numeric formats like integers or floating-point values. This transformation facilitates their use in machine learning models and is termed as **feature extraction** or **vectorization**.\n",
        "\n",
        "The scikit-learn library in Python simplifies this procedure by providing intuitive tools for both tokenization and feature extraction. In this section, we'll delve deep into how you can ready your text data for predictive modeling harnessing the power of scikit-learn. Upon completion, you'll be adept at:\n",
        "\n",
        "- Transforming text into word count vectors using **CountVectorizer**.\n",
        "- Transitioning text into word frequency vectors via **TfidfVectorizer**.\n"
      ],
      "metadata": {
        "id": "7TptADporVv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CountVectorizer in Scikit-learn: A Simplified Guide"
      ],
      "metadata": {
        "id": "cVMc5tIvuBKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The **CountVectorizer** in **``scikit-learn``** offers a straightforward mechanism to achieve three core objectives with text data:\n",
        "\n",
        "- tokenize a set of text documents\n",
        "- establish a vocabulary of identified words\n",
        "-  and subsequently, encode fresh documents leveraging this vocabulary.\n",
        "\n",
        "\n",
        "Here's a concise step-by-step on its usage:\n",
        "\n",
        "- Instantiate the **``CountVectorizer``** class.\n",
        "- Utilize the **``fit()``** method to derive a vocabulary from your **text corpus**.\n",
        "- As and when required, use the **``transform()``** method to vectorize documents based on the previously established vocabulary.\n",
        "\n",
        "> The **outcome** is an encoded vector whose length corresponds to the entire vocabulary.\n",
        "\n",
        "Each entry in this vector signifies the occurrence count of its respective word in the document. Given that many of these counts are zero (the word doesn't appear in the document), **such vectors are typically spars**.\n",
        "\n",
        "Python's **``scipy.sparse``** package facilitates efficient handling of these sparse vectors.\n",
        "\n",
        "Notably, the vectors produced post **``transform()``** are sparse. If you wish to visualize these vectors for a more intuitive understanding, you can revert them to standard NumPy arrays using the **``toarray()``** method.\n",
        "\n",
        "Here's a hands-on example showcasing the utility of **``CountVectorizer``** for tokenization, vocabulary creation, and document encoding:"
      ],
      "metadata": {
        "id": "8pW1HTlQuYn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(type(vector))\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "id": "1i4YM8JSqfg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2,\n",
        "'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
        "\n",
        "(1, 8)\n",
        "\n",
        "<class 'scipy.sparse._csr.csr_matrix'>\n",
        "\n",
        "[[1 1 1 1 1 1 1 2]]\n",
        "```\n"
      ],
      "metadata": {
        "id": "G_9blRWLleIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From our observations, it's evident that the tokenization process, by default, **converts all words to lowercase and disregards punctuation**.\n",
        "\n",
        "While these are the standard settings, the tokenization process offers various customizable options.\n",
        "\n",
        "> I strongly recommend perusing the API documentation to familiarize yourself with these configurations.\n",
        "\n",
        "Upon executing the example, the vocabulary is displayed initially, followed by the dimensions of the encoded document.\n",
        "\n",
        "> The vocabulary consists of 8 unique words, which means the encoded vectors span a length of 8.\n",
        "\n",
        "Notably, **the encoded vector manifests as a sparse matrix**. When viewed in its array format, it's evident that each word, with the exception of **``the``** (indexed as 7), has been counted once. The word **``the``** registers a count of 2."
      ],
      "metadata": {
        "id": "V6pi3anrqzD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> It's crucial to note that if a document contains words not present in the established vocabulary, the vectorizer will still function.\n",
        "\n",
        "**Words not found in the vocabulary are simply disregarded**, and their counts in the resultant vector are omitted. To illustrate, let's use the aforementioned vectorizer to encode a document comprising both a word from the vocabulary and a word outside of it."
      ],
      "metadata": {
        "id": "trMtYibbvsMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode another document\n",
        "text2 = [\"the puppy\"]\n",
        "vector = vectorizer.transform(text2)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "id": "xNoYJoyywBSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When executing this example, the output is the array representation of the encoded **sparse vector**. It indicates a single occurrence of the word present in the vocabulary, while the word absent from the vocabulary is entirely overlooked."
      ],
      "metadata": {
        "id": "TDDwLNOmwFRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Frequencies with TfidfVectorizer"
      ],
      "metadata": {
        "id": "r51kL8qpwOHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the realm of text data analysis, word counts offer a rudimentary technique for representing textual information. However, a salient challenge with relying solely on raw word counts is that commonplace words—often referred to as **``stop words``** like **``the``**—typically emerge frequently.\n",
        "\n",
        "Consequently, these high frequencies might inadvertently overshadow more pertinent terms in the document, rendering the vectors less informative. To address this concern, a more sophisticated measure, known as **Term Frequency-Inverse Document Frequency (TF-IDF)**, has been propounded.\n",
        "\n",
        "**TF-IDF**, an abbreviation for Term Frequency-Inverse Document Frequency, **assigns a weight** to each word in a document based on two key metrics:\n",
        "\n",
        "- **Term Frequency (TF)**: This metric computes the recurrence of a word within a specific document. Essentially, it provides a measure of the word's significance within that individual document.\n",
        "- **Inverse Document Frequency (IDF)**: This metric serves to attenuate the weight of words that manifest ubiquitously across multiple documents, thereby ensuring that words that are pervasive and perhaps less meaningful do not unduly dominate.\n",
        "\n",
        "To elucidate further without delving into intricate mathematical derivations, TF-IDF scoring aims to accentuate words that hold significant value within a particular document but aren't necessarily prevalent across an entire corpus of documents.\n",
        "\n",
        "The **``TfidfVectorizer``** in Python serves a dual purpose:\n",
        "\n",
        "1. It not only tokenizes documents but also discerns the vocabulary and computes the IDF weightings.\n",
        "2. Subsequently, it facilitates the encoding of novel documents in accordance with this learned knowledge.\n",
        "\n",
        "For instances where a pre-existing **``CountVectorizer``** has been utilized, the **``TfidfTransformer``** can be seamlessly integrated to solely determine the IDF values and subsequently embark on the encoding process. The procedural steps encompassing creation, fitting, and transformation, akin to the **``CountVectorizer``**, remain consistent.\n",
        "\n",
        "The subsequent example delineates the procedure for employing the **``TfidfVectorizer``**, wherein it learns both the vocabulary and IDF weightings across a trifecta of succinct documents and subsequently encodes one amongst them."
      ],
      "metadata": {
        "id": "z7_rWCLDx3DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "        \"The dog.\",\n",
        "        \"The Fox\"]\n",
        "\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "\n",
        "# gives us a mapping of each term to its index in the vector\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# provides the computed inverse document frequency (IDF) values for each term\n",
        "print(vectorizer.idf_)\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform([text[0]])\n",
        "\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "id": "1XxIgZOLAvaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.transform([text[1]]).toarray()"
      ],
      "metadata": {
        "id": "3f-asKyV9vf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.transform([text[2]]).toarray()"
      ],
      "metadata": {
        "id": "x-xiSfhy-J3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mathematical Breakdown of TF-IDF**\n",
        "\n",
        "**1. Term Frequency (TF)**\n",
        "\n",
        "The term frequency for a term `t` in a document `d` is defined as:\n",
        "\n",
        "$\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}$\n",
        "\n",
        "**2. Inverse Document Frequency (IDF)**\n",
        "\n",
        "The inverse document frequency for a term `t` is defined as:\n",
        "\n",
        "$\\text{IDF}(t) = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t} \\right) + 1$\n",
        "\n",
        "**3. TF-IDF**:\n",
        "The TF-IDF score is the product of TF and IDF.\n",
        "\n",
        "$TFIDF(t, d) = TF(t, d) \\times IDF(t)$\n"
      ],
      "metadata": {
        "id": "wUn7KB65G4DU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note:\n",
        "\n",
        "In practice, TF-IDF implementations might involve additional steps, like normalization and stopword removal. The scikit-learn **``TfidfVectorizer``** does some of these for you."
      ],
      "metadata": {
        "id": "5HiqZ7BwNMAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Prepare Text Data With Keras"
      ],
      "metadata": {
        "id": "yg9iX-4B4iDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of **deep learning**, raw text cannot be directly utilized. Instead, it needs to be converted into numerical format, like word embeddings, to be effectively employed as input or output for machine learning and deep learning algorithms. The Keras library comes equipped with fundamental tools for this purpose. In this section, you'll delve into the ways Keras aids in text data preparation. Upon completion, you'll be familiar with:\n",
        "\n",
        "- Handy methods provided by Keras for streamlining text data preparation.\n",
        "- The Tokenizer API, which can be tailored based on training data and subsequently employed to encode training, validation, and test data.\n",
        "- Four distinct document encoding methods available via the Tokenizer API."
      ],
      "metadata": {
        "id": "f3dtfBgl4ngr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Text to Individual Words Using the ``text_to_word_sequence`` Method."
      ],
      "metadata": {
        "id": "vsQPM_IO47Uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with text, an initial beneficial approach is to segment it into individual words. These individual words are referred to as tokens, and the act of dividing text into these tokens is termed tokenization. Keras offers the **``text_to_word_sequence()``** method to assist in breaking down text into word lists. By its default settings, the function performs three actions:\n",
        "\n",
        "- Divides words by spaces.\n",
        "- Eliminates punctuation.\n",
        "- Transforms the text into lowercase using the parameter (lower=True).\n",
        "\n",
        "These default behaviors can be modified by providing specific arguments to the method. Here's a demonstration of how the **``text_to_word_sequence()``** function can be employed to segment a given text (in this context, a straightforward string) into its constituent words."
      ],
      "metadata": {
        "id": "hRIQjXMA7CoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "0kYZ-pwS6irk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This initial step is beneficial, yet additional pre-processing is essential before the text is ready for use."
      ],
      "metadata": {
        "id": "OyjaLi9q6p0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding with one_hot"
      ],
      "metadata": {
        "id": "xU-mAkV7--RW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representing a document as a series of unique integer values for each word is a common approach. Keras offers the **``one_hot()``** function which enables tokenization and integer encoding of a text document simultaneously.\n",
        "\n",
        "> Contrary to what its name implies, this function doesn't produce a one-hot encoding of the document.\n",
        "\n",
        "Instead, it acts as a **wrapper** for the **hashing_trick()** function, which will be discussed subsequently. This function outputs an integer-encoded version of the document.\n",
        "\n",
        "> Due to the utilization of a hash function, **there might be instances of collisions**, implying that some words might not get distinct integer values.\n",
        "\n",
        "Similar to the **``text_to_word_sequence()``** function mentioned earlier, **``one_hot()``** converts the text to lowercase, removes punctuation, and separates words based on spaces.\n",
        "\n",
        "Furthermore, when employing this function, the vocabulary size (total unique words) has to be defined. This count could encompass the total words in the document or even more, especially if you plan on encoding other documents with additional words. The vocabulary's size determines the hash space. By default, the hash function is utilized. However, as we'll delve deeper in the subsequent section, different hash functions can be chosen when directly invoking the **``hashing_trick()``** function.\n",
        "\n",
        "To dissect the document into individual words, we can utilize the **``text_to_word_sequence()``** function from earlier and then employ a **``set``** to display only the distinct words within the document. The count of this set can provide an estimate of the vocabulary size for a single document. For instance:"
      ],
      "metadata": {
        "id": "dnsP91gB_AJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "1lHGTd8Q_4dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can integrate this with the **``one_hot()``** function to encode the words within the document. The entire example is provided below. To reduce the likelihood of collisions during word hashing, the vocabulary size has been expanded by one-third."
      ],
      "metadata": {
        "id": "E3mOci83ADPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "\n",
        "# integer encode the document\n",
        "result = one_hot(text, round(vocab_size*1.3))\n",
        "print(result)"
      ],
      "metadata": {
        "id": "n76BN_QLAKVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hash Encoding with ``hashing_trick``"
      ],
      "metadata": {
        "id": "IG8304-y8-JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Integer and count-based encodings have a constraint: they necessitate the maintenance of a vocabulary along with its corresponding integer mappings.\n",
        "\n",
        "One way to circumvent this is by employing a **``one-way hash function``** to transform words into integers. This method eliminates the need for vocabulary tracking, making it quicker and more memory-efficient.\n",
        "\n",
        "Keras offers the **``hashing_trick()``** function, which not only tokenizes but also assigns integer values to the document, akin to the **``one_hot()``** function. It's a versatile tool; you can choose the hash function, with \"hash\" as the default. Other options include the built-in **``md5``** function or even custom-made functions. Here's a demonstration of how a document can be integer-encoded using the **``md5``** hash function."
      ],
      "metadata": {
        "id": "jLSlqRAM9QTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import hashing_trick\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "\n",
        "# integer encode the document\n",
        "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
        "print(result)"
      ],
      "metadata": {
        "id": "P6kexPBx7eEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer API"
      ],
      "metadata": {
        "id": "7_AhEoH38zXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Up to this point, we have explored straightforward convenience methods in **``Keras``** for text preparation. However, **``Keras``** offers a more **``advanced API``** for text preparation, which can be employed effectively across multiple text documents.\n",
        "\n",
        "This approach is particularly well-suited for substantial projects. In **``Keras``**, the **``Tokenizer``** class is provided for the purpose of text document preparation in the context of deep learning. To utilize the **``Tokenizer``**, it must first be initialized and then applied to either raw text documents or integer-encoded text documents. For instance:\n",
        "\n",
        "```python\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = ['Well done!','Good work','Great effort','nice work','Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)\n",
        "```"
      ],
      "metadata": {
        "id": "r8f_O-j4czka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once initialized, the **``Tokenizer``** offers four attributes that enable you to retrieve information about your documents:\n",
        "\n",
        "- **Word Counts**: This attribute provides a dictionary that maps words to their respective occurrence counts as observed during the Tokenizer initialization.\n",
        "- **Word Docs**: You can access another dictionary that associates words with the number of documents they appear in.\n",
        "- **Word Index**: The Tokenizer generates a dictionary that assigns unique integers to each word in your documents.\n",
        "- **Document Count**: This attribute supplies a dictionary that indicates the number of documents in which each word appears, a calculation made during the initialization process.\n",
        "\n",
        "```python\n",
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n",
        "```"
      ],
      "metadata": {
        "id": "oSxx2WGjd8UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# define 5 documents\n",
        "docs = ['Well done!',\n",
        "        'Good work',\n",
        "        'Great effort',\n",
        "        'nice work',\n",
        "        'Excellent!']\n",
        "\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)\n",
        "\n",
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)"
      ],
      "metadata": {
        "id": "CAilnzUIaqxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the **``Tokenizer``** has been trained on the training data, you can employ it to encode documents within both the training and test datasets. The **``texts_to_matrix()``** function provided by the **``Tokenizer``** enables the creation of one vector for each document in the input. The length of these vectors corresponds to the total vocabulary size.\n",
        "\n",
        "This function offers a range of standard text encoding schemes commonly used in **``bag-of-words``** models. You can specify your desired encoding scheme by providing a **``mode``** argument to the function. The available modes are as follows:\n",
        "\n",
        "1. **Binary**: This mode indicates whether each word is present or absent in the document. It is the default encoding scheme.\n",
        "\n",
        "2. **Count**: In this mode, the encoding represents the count of each word in the document.\n",
        "\n",
        "3. **TF-IDF (Text Frequency-Inverse Document Frequency)**: This mode calculates the TF-IDF score for each word in the document, providing a measure of the word's importance within the document and across the entire corpus.\n",
        "\n",
        "4. **Frequency (Freq)**: The frequency mode represents the frequency of each word as a ratio of its occurrences to the total number of words within each document."
      ],
      "metadata": {
        "id": "YO17GEVjfLBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding represents the count of each word in the document\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "id": "z6v25ThYcRsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# indicates whether each word is present or absent in the document\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='binary')\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "id": "a_S2tCfxebru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculates the TF-IDF score for each word in the document\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='tfidf')\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "id": "vJ4b6TyIfb6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frequency of each word as a ratio of its occurrences\n",
        "# to the total number of words within each document\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='freq')\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "id": "xZUCYrYBffjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words"
      ],
      "metadata": {
        "id": "QUVtM24ssM-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bag-of-words model serves as a method for representing text data when employing machine learning algorithms for text-related tasks.\n",
        "\n",
        "It is characterized by its simplicity in both comprehension and implementation, and it has proven to be highly effective in various applications, including language modeling and document classification.\n",
        "\n",
        "This section aims to introduce you to the bag-of-words model as a feature extraction technique in the field of natural language processing. By the end of this section, you will gain insights into the following key aspects:\n",
        "\n",
        "- **Understanding the bag-of-words model** and recognizing its significance in text representation.\n",
        "- **Developing** a bag-of-words model for a collection of documents.\n",
        "- **Utilizing** various techniques to construct a vocabulary and assign scores to words within the model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Qa92ksEsOwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Challenge with Text Data"
      ],
      "metadata": {
        "id": "O2ZSINqRsmPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "One of the challenges associated with modeling text data is its **inherent messiness**. Machine learning algorithms, in particular, thrive on structured, fixed-length inputs and outputs. They are not equipped to handle raw text directly; instead, text must undergo a transformation into numerical representations, specifically, vectors of numbers.\n",
        "\n",
        "In the realm of natural language processing, these vectors (represented as 'x') are derived from textual data to capture various linguistic characteristics of the text. This process is commonly known as **feature extraction** or **feature encoding**. One widely used and straightforward approach for **feature extraction** with text data is referred to as the **``bag-of-words``** model."
      ],
      "metadata": {
        "id": "hzAG3tFMs-KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Bag-of-Words Model"
      ],
      "metadata": {
        "id": "mg7Cxmf3tVLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The **Bag-of-Words (BoW)** model, a fundamental concept in text analysis, serves as a robust method for **feature extraction** from textual data, particularly in the context of machine learning algorithms. This approach is renowned for its inherent simplicity and adaptability, offering a versatile framework for feature extraction from diverse documents.\n",
        "\n",
        "Essentially, a bag-of-words represents text by elucidating the presence of words within a given document, relying on two pivotal elements:\n",
        "\n",
        "- **A Vocabulary of Known Words**: This component encompasses a collection of words that are recognized within the context of the analysis.\n",
        "- **A Measure of the Presence of Known Words**: The BoW model quantifies the occurrence of these known words within the document.\n",
        "\n",
        "The nomenclature **``bag-of-words``** is aptly chosen, signifying the deliberate disregard for any information pertaining to the arrangement or structure of words within the document. Instead, the model's primary concern lies in ascertaining whether known words are present in the document, without regard to their specific location within the text.\n",
        "\n",
        "A prevalent feature extraction technique for both sentences and documents is the Bag-of-Words approach (BoW). This method involves the construction of a histogram, effectively treating each word count as an individual feature.\n",
        "\n",
        "> The underlying intuition of the BoW model rests on the premise that documents sharing similar content tend to exhibit analogous characteristics. Furthermore, it posits that by scrutinizing content in isolation, meaningful insights about the document's semantic content can be gleaned.\n",
        "\n",
        "It is important to note that the complexity of the Bag-of-Words model can be tailored to specific needs. This complexity manifests in the decision-making processes surrounding the design of the vocabulary of known words (or tokens) and the methodology employed to evaluate the presence of these known words. Both of these considerations warrant careful examination and will be explored in greater detail."
      ],
      "metadata": {
        "id": "5zPkVcjRtxM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Illustration of the Bag-of-Words Model\n"
      ],
      "metadata": {
        "id": "8QUdIMChuPtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To provide a tangible demonstration of the Bag-of-Words model, we will walk through a practical example.\n",
        "\n",
        "**Step 1: Data Collection**\n",
        "\n",
        "For this demonstration, we will begin by excerpting the initial lines of text from the renowned literary work **``A Tale of Two Cities``** authored by *Charles Dickens*. These excerpts have been sourced from **Project Gutenberg**.\n",
        "\n",
        "```python\n",
        "It was the best of times,\n",
        "it was the worst of times,\n",
        "it was the age of wisdom,\n",
        "it was the age of foolishness,\n",
        "```"
      ],
      "metadata": {
        "id": "bOK6CXi_ujfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this compact example, we will consider each line as an individual document, and collectively, these four lines constitute our entire corpus of documents."
      ],
      "metadata": {
        "id": "Hyp5al7IurAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Vocabulary Construction**\n",
        "\n",
        "We can proceed by creating a list that encompasses all the words within our model's vocabulary. In this context, we consider the unique words, disregarding variations in case and excluding punctuation.\n",
        "\n",
        "```python\n",
        "it\n",
        "was\n",
        "the\n",
        "best\n",
        "of\n",
        "times\n",
        "worst\n",
        "age\n",
        "wisdom\n",
        "foolishness\n",
        "```\n",
        "\n",
        "This results in a vocabulary of 10 words, derived from a corpus consisting of a total of 24 words."
      ],
      "metadata": {
        "id": "S4M_HvEDvEaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Generating Document Vectors**\n",
        "\n",
        "The subsequent phase entails assigning scores to the words contained within each document. The objective is to transform each free-text document into a vector, which can serve as input or output for a machine learning model. Given our knowledge of the vocabulary comprising 10 words, we can establish a fixed-length representation of each document, consisting of 10 positions within the vector to score each word.\n",
        "\n",
        "The most straightforward scoring technique involves denoting the presence or absence of words as binary values: 0 for absence and 1 for presence.\n",
        "\n",
        "Employing the arbitrary sequence of words provided earlier in our vocabulary, we can systematically process the initial document, **``It was the best of times,``** and translate it into a binary vector. The scoring for this document would manifest as follows:\n",
        "\n",
        "```python\n",
        "it = 1\n",
        "was = 1\n",
        "the = 1\n",
        "best = 1\n",
        "of = 1\n",
        "times = 1\n",
        "worst = 0\n",
        "age = 0\n",
        "wisdom = 0\n",
        "foolishness = 0\n",
        "```\n",
        "\n",
        "As a binary vector, this would look as follows:\n",
        "\n",
        "```python\n",
        "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "```"
      ],
      "metadata": {
        "id": "PfMj0hwdvSne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other three documents would look as follows:\n",
        "\n",
        "```python\n",
        "\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
        "\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
        "\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
        "```\n",
        "\n",
        "All word orderings are uniformly disregarded, ensuring a consistent method for extracting features from any document within our corpus, thereby rendering them suitable for modeling purposes. Even when confronted with new documents that partially overlap with the established vocabulary of known words but may also include words outside of this vocabulary, they can still be encoded effectively. In such cases, only the occurrences of known words are assigned scores, while unknown words are omitted from consideration. This approach demonstrates its scalability, making it well-suited for handling extensive vocabularies and more substantial documents."
      ],
      "metadata": {
        "id": "1rNHE_ZYvxrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Managing the Vocabulary\n"
      ],
      "metadata": {
        "id": "HoWCOVzQwMVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> As the size of the vocabulary expands, the vector representation of documents grows proportionally.\n",
        "\n",
        "In the previous example, the length of the document vector equated to the number of known words. When dealing with a considerably extensive corpus, such as thousands of books, it is conceivable that the vector's length may extend to thousands or even millions of positions. Additionally, **it's common for each document to contain only a small subset of the known words from the vocabulary**.\n",
        "\n",
        "This phenomenon results in a vector that contains numerous zero scores, characterized as a **sparse vector** or **sparse representation**. Sparse vectors consume more memory and computational resources during modeling, and the substantial number of positions or dimensions can pose considerable challenges for traditional algorithms. Consequently, there is a compelling incentive to reduce vocabulary size when employing a bag-of-words model.\n",
        "\n",
        "There exist uncomplicated text preprocessing techniques that can serve as an initial step in this endeavor, including:\n",
        "\n",
        "- Ignoring letter case.\n",
        "- Disregarding punctuation.\n",
        "- Omitting frequently occurring words with low information content, known as stop words (e.g., \"a,\" \"of,\" etc.).\n",
        "- Correcting misspelled words.\n",
        "- Reducing words to their root form, known as **stemming**, using appropriate stemming algorithms.\n",
        "\n",
        "> For a more intricate strategy, one can opt to construct a vocabulary of grouped words.\n",
        "\n",
        "This approach not only alters the scope of the vocabulary but also allows the bag-of-words model to capture a modicum of additional semantic meaning from the documents. In this context, each word or token is referred to as a **``gram``**.\n",
        "\n",
        "> Formulating a vocabulary of two-word pairs is specifically known as a **bigram model**.\n",
        "\n",
        "It's important to note that only the bigrams that manifest within the corpus are modeled, rather than considering all possible bigrams.\n",
        "\n",
        "> An **n-gram** represents a sequence of **``n tokens``** within a text. To illustrate, a 2-gram, often referred to as a **bigram**, consists of two-word sequences such as **``please turn``**, **``turn your``**, or **``your homework``**. On the other hand, a **3-gram**, commonly known as a **``trigram``**, encompasses three-word sequences like **``please turn your``** or **``turn your homework``**.\n",
        "\n",
        "As an illustration, consider the bigrams within the initial line of text presented in the preceding section, **``It was the best of times``**. These bigrams are as follows:\n",
        "\n",
        "```python\n",
        "it was\n",
        "was the\n",
        "the best\n",
        "best of\n",
        "of times\n",
        "```\n",
        "\n",
        "> A representation based on a **``bag-of-bigrams``** exhibits significantly greater potency than the traditional bag-of-words approach, and in numerous instances, it proves exceptionally challenging to surpass in performance."
      ],
      "metadata": {
        "id": "f52PVeqlwynn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Word Occurrences\n"
      ],
      "metadata": {
        "id": "M_7aT8Oox336"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After the selection of a vocabulary, the next crucial step involves scoring the occurrence of words within sample documents. In the previously demonstrated example, we have already encountered one straightforward scoring method: binary scoring, which involves marking the presence or absence of words. Additionally, there are several other uncomplicated scoring approaches, including:\n",
        "\n",
        "- **Counts**: This method involves tallying the number of times each word appears within a document.\n",
        "- **Frequencies**: Here, we calculate the frequency of each word's occurrence within a document relative to the total number of words in that document."
      ],
      "metadata": {
        "id": "CoQ3v0ySzPKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constraints of the Bag-of-Words Model\n"
      ],
      "metadata": {
        "id": "3MIa_LH7zWLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The bag-of-words model, while distinguished for its simplicity and adaptability when it comes to customizing text data, has achieved significant success in various prediction tasks, including language modeling and document classification. Nevertheless, it is not without its limitations, which encompass the following aspects:\n",
        "\n",
        "- **Vocabulary**: The design of the vocabulary necessitates meticulous consideration, especially concerning its size. The vocabulary size directly impacts the sparsity of document representations.\n",
        "\n",
        "- **Sparsity**: Sparse representations pose formidable challenges, both in terms of computational complexity (space and time) and information utilization. The difficulty lies in effectively harnessing limited information within an expansive representational space.\n",
        "\n",
        "- **Semantic Meaning**: Disregarding word order, the bag-of-words model overlooks the context and, consequently, the semantics of words within a document. The context and meaning can contribute substantially to the model's understanding, potentially distinguishing between differently arranged words (e.g., **``this is interesting``** vs. **``is this interesting``**), identifying synonyms (e.g., **``old bike``** vs. **``used bike``**), and addressing a myriad of other linguistic nuances."
      ],
      "metadata": {
        "id": "WprYVmqozizU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Resources: Exploring the Development of Deep Learning Models with Keras"
      ],
      "metadata": {
        "id": "UfJAIUgXXpBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Creating and evaluating deep learning neural networks becomes intuitive in Python using Keras, but it's crucial to adhere to a specific model life-cycle. In this lesson, we will walk you through the detailed **``life-cycle``** steps for formulating, training, and assessing deep learning neural networks in Keras. Furthermore, we'll guide you on predicting using a trained model and delve into the functional API for more versatile model design.\n",
        "\n",
        "By the end of this section, you will have insights into:\n",
        "\n",
        "- Defining, compiling, fitting, and evaluating a deep learning neural network using Keras.\n",
        "- Adopting standard defaults for both regression and classification predictive modeling tasks.\n",
        "- Utilizing the functional API to craft standard Multilayer Perceptrons, convolutional, and recurrent neural networks."
      ],
      "metadata": {
        "id": "pPNSViDXYArn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Neural Network Model in Keras\n"
      ],
      "metadata": {
        "id": "9GpXpx4zMQzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. Here's a step-by-step guide on how to create a neural network model using Keras:\n",
        "\n",
        "**1. Define the Network**\n",
        "\n",
        "First, you need to define the architecture of your neural network. This involves specifying the number of layers, the number of neurons in each layer, and the activation function for each neuron.\n",
        "\n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=8, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "```\n",
        "\n",
        "\n",
        "**2. Compile the Network**\n",
        "\n",
        "After defining the architecture, you need to compile the model. This step involves specifying the optimizer, loss function, and evaluation metric(s).\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "**3. Fit the Network**\n",
        "\n",
        "Now, you can train the model using your training data. This involves feeding the input data and the corresponding target values to the model, specifying the number of epochs, and optionally, the batch size.\n",
        "\n",
        "```python\n",
        "# Assuming X_train and y_train are your input data and labels respectively\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "```\n",
        "\n",
        "**4. Evaluate the Network**\n",
        "\n",
        "After training, you should evaluate the performance of your model using a separate dataset (often called the test dataset).\n",
        "\n",
        "```python\n",
        "# Assuming X_test and y_test are your test data and labels\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "```\n",
        "\n",
        "**5. Make Predictions**\n",
        "\n",
        "Finally, you can use the trained model to make predictions on new, unseen data.\n",
        "\n",
        "```python\n",
        "# Assuming X_new is your new data\n",
        "predictions = model.predict(X_new)\n",
        "```\n",
        "\n",
        "And that's it! You've successfully created, trained, and evaluated a neural network model in Keras.\n",
        "\n"
      ],
      "metadata": {
        "id": "GmNvJnq1Yfur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras Functional Models"
      ],
      "metadata": {
        "id": "pgpWV6ZVVBpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Keras, the functional API provides a more flexible approach to defining models compared to the sequential API. While the sequential API allows you to stack layers in a linear fashion, the functional API allows you to define more complex architectures, such as multi-input, multi-output, and shared layers models."
      ],
      "metadata": {
        "id": "YDXS9VSn0NSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basics of Functional API"
      ],
      "metadata": {
        "id": "DmRYG1dw0afu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of starting with a **``Sequential model``**, you'll begin by defining placeholder tensors for your inputs. You'll then define a series of layer calls, treating these layers as functions that process the input tensors and generate output tensors.\n",
        "\n",
        "**Example: Simple Feedforward Network**\n",
        "\n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_shape=(784,), activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "```\n"
      ],
      "metadata": {
        "id": "XTqhI6IF0f97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the functional API, the same model can be defined as:\n",
        "\n",
        "```python\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "inputs = Input(shape=(784,))\n",
        "x = Dense(32, activation='relu')(inputs)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "```\n",
        "\n",
        "Notice how in the functional approach, the layers are used as functions that take tensors and return tensors."
      ],
      "metadata": {
        "id": "XAlJtUH40zAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Multi-input Model**\n",
        "\n",
        "Imagine you want a model with two different inputs. For instance, one input could be an image, and the other input could be a description of that image.\n",
        "\n",
        "```python\n",
        "from keras.layers import Input, Dense, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "# Image input\n",
        "image_input = Input(shape=(128,128,3))\n",
        "x1 = Dense(128, activation='relu')(image_input)\n",
        "\n",
        "# Text input\n",
        "text_input = Input(shape=(100,))\n",
        "x2 = Dense(32, activation='relu')(text_input)\n",
        "\n",
        "# Merge the outputs of the two branches\n",
        "merged = concatenate([x1, x2])\n",
        "\n",
        "# Final dense layer\n",
        "output = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "# Model\n",
        "model = Model(inputs=[image_input, text_input], outputs=output)\n",
        "```\n",
        "\n",
        "This kind of architecture allows for flexibility in feeding different kinds of data into the model and processing them differently."
      ],
      "metadata": {
        "id": "vh4hapXJ0-Vn"
      }
    }
  ]
}