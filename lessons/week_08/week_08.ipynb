{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Construct a Neural Bag-of-Words Framework for Evaluating Sentiments"
      ],
      "metadata": {
        "id": "5vueTEIX60kM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movie reviews can be categorized as either positive or negative. Analyzing the sentiment of movie review text is commonly referred to as sentiment analysis. One common approach to build sentiment analysis models is through the bag-of-words method. This method converts documents into vector formats where each word is given a specific score.\n",
        "\n",
        "In this section, you'll learn:\n",
        "\n",
        "- The process of formatting review text data for modeling using a limited vocabulary.\n",
        "- Techniques to employ the bag-of-words model for training and testing datasets.\n",
        "- Steps to create a Multilayer Perceptron with the bag-of-words model and how to utilize it for predictions on fresh review text data."
      ],
      "metadata": {
        "id": "zEmSPHxD61yD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "VQeg_J4p7I75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of preparing the movie review dataset was initially outlined in the previous section. In this segment, we'll focus on the following tasks:\n",
        "\n",
        "- Dividing the data into train and test subsets.\n",
        "- Importing the data and cleansing it by eliminating punctuation and numerals.\n",
        "- Establishing a defined list of desired words."
      ],
      "metadata": {
        "id": "iqhvdXl08x5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Test Split"
      ],
      "metadata": {
        "id": "jR8V6GPc86GN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Let's assume we're working on a system designed to predict the sentiment of textual movie reviews, categorizing them as either **``positive``** or **``negative``**.\n",
        "\n",
        "Once our model is established, we will predict sentiments of new textual reviews. Consequently, the same data preparation steps we applied to the training data will be needed for these new reviews as well.\n",
        "\n",
        "To integrate this requirement into our model evaluation,\n",
        "\n",
        "**we'll divide our datasets into training and test groups** before any data processing takes place. By doing this, we ensure that any specific knowledge from the test set, which might influence data preparation (like specific word choices), remains unknown during data preparation and model training phases.\n",
        "\n",
        "To clarify further, we'll utilize the last 100 positive and the last 100 negative reviews (totaling 200 reviews) for testing purposes. The preceding 1,800 reviews will serve as our training dataset.\n",
        "\n",
        "> This essentially divides our data into a **``90-10 ratio``** for training and testing, respectively.\n",
        "\n",
        "To facilitate this division, we can rely on the filenames of the reviews. For instance, reviews labeled from 000 to 899 are designated for training, while those labeled 900 and beyond are reserved for testing."
      ],
      "metadata": {
        "id": "YbWq3llf9naR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Cleaning Reviews"
      ],
      "metadata": {
        "id": "jjckf4eDud0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text data appears to be quite **``tidy``** already, so there isn't much preparation needed. Without delving too deeply into the specifics, we'll prep the data using the following approach:\n",
        "\n",
        "- Tokenize the text based on white spaces.\n",
        "- Purge all punctuation from the words.\n",
        "- Discard any words that aren't solely made up of alphabetical characters.\n",
        "- Eliminate all known stop words from the text.\n",
        "- Exclude any words that are of length 1 character.\n",
        "\n",
        "We can encapsulate all of these steps into a function named **``clean_doc()``**, which will accept the raw text extracted from a file as an argument and return a list of cleaned tokens. Additionally, we can create a function named **``load_doc()``** to fetch a document from a file, making it ready for use with the **``clean_doc()``** function. Below is an example demonstrating how to clean the first positive review using these functions."
      ],
      "metadata": {
        "id": "cFNr6XGsu5it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=10OsDrN-m2IIKqZJrf-xMbEtg8VlGJ2DQ\n",
        "!tar -xvf review_polarity.tar.gz"
      ],
      "metadata": {
        "id": "4Nv6jrrEuFaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "FfZ44b2l50AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load the document\n",
        "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "nS1h1KCL9s75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Establishing a Vocabulary\n",
        "\n"
      ],
      "metadata": {
        "id": "lKc6_oCsvYTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**``Setting up a vocabulary``** of recognized terms is crucial when employing a bag-of-words model.\n",
        "\n",
        "A larger number of words results in a more extensive representation of documents, hence it's vital to limit the words to only those presumed to be indicative. Determining this upfront is challenging and it's often necessary to explore various assumptions regarding the formation of a beneficial vocabulary.\n",
        "\n",
        "As seen earlier, it's possible to exclude punctuation and numbers from the vocabulary. This procedure can be replicated across all documents to generate a collection of all recognized words.\n",
        "\n",
        "A vocabulary can be structured as a **``Counter``**, which is a dictionary-like mapping of words alongside their frequency, facilitating straightforward updates and inquiries.\n",
        "\n",
        "Each document can be integrated into the counter via a new function named **``add_doc_to_vocab()``**. Additionally, a new function named **``process_docs()``** can be utilized to traverse through all reviews in the negative directory followed by the positive directory. The comprehensive example is provided below."
      ],
      "metadata": {
        "id": "env4Z2hdgECH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "metadata": {
        "id": "MMW2GuzLgV2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the example reveals a vocabulary comprising **``44,276 words``**. Additionally, we can observe a snippet of the top 50 most frequently utilized words in the movie reviews. It's noteworthy that this vocabulary was assembled solely from the reviews present in the training dataset.\n",
        "\n",
        "\n",
        "```python\n",
        "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262),\n",
        "('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844),\n",
        "('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703),\n",
        "('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511),\n",
        "('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288),\n",
        "('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238),\n",
        "('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131),\n",
        "('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024),\n",
        "('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952),\n",
        "('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n",
        "```"
      ],
      "metadata": {
        "id": "htPHiKrqg8v8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can sift through the vocabulary, discarding words with a low frequency, such as those appearing only once or twice across all reviews. For instance, the snippet below will filter out and retain only the tokens that have a frequency of 2 or more across all reviews."
      ],
      "metadata": {
        "id": "7LUh6YGxhkxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "\t# convert lines to a single blob of text\n",
        "\tdata = '\\n'.join(lines)\n",
        "\t# open file\n",
        "\tfile = open(filename, 'w')\n",
        "\t# write text\n",
        "\tfile.write(data)\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# keep tokens with a min occurrence\n",
        "min_occurrence = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
        "print(len(tokens))\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "metadata": {
        "id": "PuYaOap0kaFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the above example with this modification demonstrates that the vocabulary size reduces by slightly more than half, shrinking from around 44,000 to about 25,000 words."
      ],
      "metadata": {
        "id": "g_YEK5sVkoPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-Words Representation"
      ],
      "metadata": {
        "id": "UeU6cXeskyco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll explore the process of transforming each review into a format suitable for a Multilayer Perceptron model.\n",
        "\n",
        "> The **``bag-of-words``** model serves as a mechanism for **extracting features from text**, enabling its utilization with machine learning algorithms like neural networks.\n",
        "\n",
        "Each document, in this context a review, is transitioned into a vector representation. The vector's item count aligns with the vocabulary's word count, implying a longer vector representation with a larger vocabulary, hence the prior section's inclination towards smaller vocabularies. The introduction to the bag-of-words model was made in last week.\n",
        "\n",
        "Within a document, words are evaluated and their scores are positioned in the corresponding spot within the representation. The upcoming section will delve into various word scoring methodologies. In this segment, the focus is on transmuting reviews into vectors, primed for the training of an initial neural network model. This segment unfolds in two phases:\n",
        "\n",
        "1. Transitioning reviews into tokenized lines.\n",
        "2. Encoding reviews utilizing a bag-of-words model representation."
      ],
      "metadata": {
        "id": "uBXdckMWk-nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Reviews into Lines of Tokens"
      ],
      "metadata": {
        "id": "gO0iEtEWmDSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can transform reviews into vectors for modeling, it's essential to tidy them up first.\n",
        "\n",
        "This entails loading the reviews, carrying out the cleaning procedure devised earlier, filtering out words not present in the selected vocabulary, and turning the remaining tokens into a single string or line prepped for encoding.\n",
        "\n",
        "Initially, we require a function to prepare an individual document. Below is the function **``doc_to_line()``** detailed, which will load a document, clean it, filter out tokens not in the vocabulary, and then return the document as a string with tokens separated by white spaces.\n",
        "\n",
        "```python\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "  # load the doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  return ' '.join(tokens)\n",
        "```"
      ],
      "metadata": {
        "id": "-Wc8wYN-mTrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following that, we require a function to process all documents in a directory (like pos and neg) to transform the documents into lines. Below, the **``process_docs()``** function is outlined, which accomplishes this task. It anticipates a directory name and a vocabulary set as input arguments, and returns a list of processed documents.\n",
        "\n",
        "```python\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  lines = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "  return lines\n",
        "```"
      ],
      "metadata": {
        "id": "gQx-B4ShmmMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can invoke the **``process_docs()``** function consistently for both positive and negative reviews to assemble a dataset comprising review text alongside their corresponding output labels, with **``0``** denoting negative and **``1``** denoting positive. The **``load_clean_dataset()``** function below embodies this behavior.\n",
        "\n",
        "```python\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "  # load documents\n",
        "  neg = process_docs('txt_sentoken/neg', vocab)\n",
        "  pos = process_docs('txt_sentoken/pos', vocab)\n",
        "  docs = neg + pos\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "return docs, labels\n",
        "```\n"
      ],
      "metadata": {
        "id": "5oSUGFxcnEWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "\treturn docs, labels\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "# load all training reviews\n",
        "docs, labels = load_clean_dataset(vocab)\n",
        "# summarize what we have\n",
        "print(len(docs), len(labels))"
      ],
      "metadata": {
        "id": "a_qzV_0bnwMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "n39Sd55xuyAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transforming Movie Reviews into Bag-of-Words Vectors"
      ],
      "metadata": {
        "id": "Qsngk4xFwhOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll employ the **``Keras API``** to transform reviews into encoded document vectors. Keras offers the **``Tokenizer``** class which can handle some of the cleaning and vocabulary definition chores we addressed in the prior section. Although it's beneficial to perform these tasks ourselves for a precise understanding of the actions and rationale, the **``Tokenizer``** class is convenient and efficiently converts documents into encoded vectors. Initially, the **``Tokenizer``** needs to be created, followed by fitting it on the text documents in the training dataset. In this scenario, these documents are the combined arrays of positive and negative lines as cultivated in the preceding section.\n",
        "\n",
        "```python\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "```"
      ],
      "metadata": {
        "id": "Uku2KkaZx9M-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This procedure establishes a uniform method to convert the vocabulary into a fixed-length vector comprising **``25,768``** elements, corresponding to the total word count in the vocabulary file **``vocab.txt``**. Subsequently, documents can be encoded utilizing the Tokenizer by invoking the **``texts_to_matrix()``** method. This function accepts both a list of documents to encode and an encoding mode, representing the technique employed to score words in the document. In this case, we specify **``freq``** to score words based on their occurrence frequency in the document. This encoding approach can be applied to both the loaded training and testing data, for instance:\n",
        "\n",
        "```python\n",
        "# encode data\n",
        "x_train = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "x_test = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "```"
      ],
      "metadata": {
        "id": "Dt2au8fOPtcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step encodes all the positive and negative reviews present in the training dataset. Following this, the **``process_docs()``** function from the preceding section requires modification to process reviews in either the test or train dataset selectively. To facilitate the loading of both training and test datasets, an is_train argument is introduced, which is utilized to determine which review file names should be skipped.\n",
        "\n",
        "```python\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  lines = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "  return lines\n",
        "```"
      ],
      "metadata": {
        "id": "eglBvgMrQ2XA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, the **``load_clean_dataset()``** function needs to be updated to accommodate the loading of either training or testing data, ensuring it returns a NumPy array.\n",
        "\n",
        "```python\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "  pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "  docs = neg + pos\n",
        "  # prepare labels\n",
        "  labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "  return docs, labels\n",
        "```"
      ],
      "metadata": {
        "id": "oparFB6HRRum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can consolidate all these steps into a single example."
      ],
      "metadata": {
        "id": "xylE9J1HRhYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, y_train = load_clean_dataset(vocab, True)\n",
        "test_docs, y_test = load_clean_dataset(vocab, False)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# encode data\n",
        "x_train = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "x_test = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "print(x_train.shape, x_test.shape)"
      ],
      "metadata": {
        "id": "ahEuTNzaOHB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "id": "evWr-h9NPf2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Executing the example displays the dimensions of both the encoded training dataset and test dataset, comprising 1,800 and 200 documents respectively, each possessing an encoding vocabulary of identical size (vector length)."
      ],
      "metadata": {
        "id": "GHEpEpq4Qbz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis Models"
      ],
      "metadata": {
        "id": "iMk-SaTuR4iz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this segment, we will construct Multilayer Perceptron (MLP) models to categorize encoded documents as either positive or negative. The models will embody straightforward feedforward network architectures, encompassing fully connected layers, termed Dense in the Keras deep learning library. This segment is organized into three sub-sections:\n",
        "\n",
        "- Initial sentiment analysis model\n",
        "- Comparison of word scoring methods\n",
        "- Rendering predictions for new reviews"
      ],
      "metadata": {
        "id": "65vCc51NVJqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Sentiment Analysis Model"
      ],
      "metadata": {
        "id": "1KmMIGsUVY9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can construct a basic MLP model to determine the sentiment of encoded critiques. The model will feature an input layer corresponding to the vocabulary size, which also matches the length of the input texts. This can be captured in a new variable named 'n_words' as given below:\n",
        "\n",
        "```python\n",
        "n_words = x_test.shape[1]\n",
        "```\n"
      ],
      "metadata": {
        "id": "OaISJVZnVhF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now set to define the network. It's worth noting that the model configuration was determined with minimal trial and error, and isn't specifically optimized for this challenge.\n",
        "\n",
        "Our design includes a single hidden layer consisting of **``50 neurons``**, complemented by a rectified linear activation function. For the output layer, we employ a solitary neuron utilizing a sigmoid activation function, which predicts either **``0``** for negative reviews or **``1``** for positive ones.\n",
        "\n",
        "For the training process, we'll use the efficient **``Adam``** variation of gradient descent and the **``binary cross-entropy``** loss function, which is apt for **``binary classification tasks``**. Additionally, we'll monitor accuracy during the training and assessment phases of the model.\n",
        "\n",
        "```python\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "    # define network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, input_shape=(n_words), activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile network\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize defined model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model\n",
        "```"
      ],
      "metadata": {
        "id": "yoVzP6dj6N9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll proceed to train the model using the training data. Given the model's compact size, it can be comfortably trained in just 10 epochs.\n",
        "\n",
        "```python\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, verbose=2)\n",
        "```\n",
        "\n",
        "After training is complete, we can assess how the model performs by making predictions on the test dataset and displaying the accuracy.\n",
        "\n",
        "```python\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n",
        "```\n"
      ],
      "metadata": {
        "id": "hPluXlH064e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "\t# define network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile network\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# summarize defined model\n",
        "\tmodel.summary()\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, y_train = load_clean_dataset(vocab, True)\n",
        "test_docs, y_test = load_clean_dataset(vocab, False)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# encode data\n",
        "x_train = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "x_test = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "\n",
        "# define the model\n",
        "n_words = x_test.shape[1]\n",
        "model = define_model(n_words)\n",
        "\n",
        "# fit network\n",
        "model.fit(x_train, y_train, epochs=10, verbose=2)\n",
        "\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "id": "UMbdW9xg7puG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model comfortably adapts to the training data in just 10 epochs, reaching an accuracy nearing 100%. When tested on the validation dataset, the model performs impressively, securing an accuracy of over 87%. This aligns with the low-to-mid 80s accuracy range cited in the original study. However, it's crucial to highlight that this isn't a direct comparison. The initial study (paper) utilized 10-fold cross-validation for gauging model proficiency, as opposed to a singular train/test division.\n",
        "\n",
        "Next, we'll explore various word scoring techniques for the bag-of-words model."
      ],
      "metadata": {
        "id": "-jgk0Zv_84c_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing Word Scoring Methods"
      ],
      "metadata": {
        "id": "Ozukl4f_-CDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **``texts_to_matrix()``** function provided by Keras' Tokenizer offers four distinct methods to score words:\n",
        "\n",
        "- **binary**: Words are marked as either present (1) or absent (0).\n",
        "- **count**: Each word's occurrence is represented as an integer count.\n",
        "- **tfidf**: Words are scored based on frequency, penalizing those that are common across all documents.\n",
        "- **freq**: Words are scored based on their occurrence frequency within a document.\n",
        "\n",
        "We can assess the performance of the previously developed model using each of these four word scoring methods. This involves creating a function that encodes the loaded documents based on a selected scoring method. This function will create the tokenizer, fit it on the training documents, and then generate the train and test encodings using the selected method. The **``prepare_data()``** function embodies this process, taking in lists of train and test documents.\n",
        "\n",
        "```python\n",
        "# prepare bag-of-words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "    # create the tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "    # fit the tokenizer on the documents\n",
        "    tokenizer.fit_on_texts(train_docs)\n",
        "    # encode training data set\n",
        "    x_train = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "    # encode training data set\n",
        "    x_test = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "\n",
        "    return x_train, x_test\n",
        "```\n"
      ],
      "metadata": {
        "id": "Q4U5u3x5-zJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need a function to assess the MLP based on a specific data encoding. Given the stochastic nature of neural networks, the same model can yield different outcomes when trained on identical data. This variability stems primarily from the random initial weights and the random ordering of data batches during minibatch gradient descent. Therefore, a single evaluation of a model might not be wholly reliable. It's more accurate to gauge the model's ability based on the average of several runs. The subsequent function, called **``evaluate_mode()``**, accepts encoded documents and gauges the MLP's efficacy. It trains the model on the training set and gauges its performance on the test set, repeating this 10 times. The function then returns a list containing accuracy scores from all these evaluations.\n",
        "\n",
        "```python\n",
        "# evaluate a neural network model\n",
        "def evaluate_mode(x_train, y_train, x_test, y_test):\n",
        "    scores = list()\n",
        "    n_repeats = 30\n",
        "    n_words = x_test.shape[1]\n",
        "    for i in range(n_repeats):\n",
        "        # define network\n",
        "        model = Sequential()\n",
        "        model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        # compile network\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        # fit network\n",
        "        model.fit(x_train, y_train, epochs=10, verbose=2)\n",
        "        # evaluate\n",
        "        loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "        scores.append(acc)\n",
        "        print('%d accuracy: %s' % ((i+1), acc))\n",
        "    return scores\n",
        "```"
      ],
      "metadata": {
        "id": "EeG9ozyq_ZLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're set to assess the performance using the four distinct word scoring techniques. Integrating all the components, the comprehensive example is provided below."
      ],
      "metadata": {
        "id": "3LqWFMpV_8qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels\n",
        "\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "\t# define network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile network\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# evaluate a neural network model\n",
        "def evaluate_mode(x_train, y_train, x_test, y_test):\n",
        "\tscores = list()\n",
        "\tn_repeats = 10\n",
        "\tn_words = x_test.shape[1]\n",
        "\tfor i in range(n_repeats):\n",
        "\t\t# define network\n",
        "\t\tmodel = define_model(n_words)\n",
        "\t\t# fit network\n",
        "\t\tmodel.fit(x_train, y_train, epochs=10, verbose=0)\n",
        "\t\t# evaluate\n",
        "\t\t_, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\t\tscores.append(acc)\n",
        "\t\tprint('%d accuracy: %s' % ((i+1), acc))\n",
        "\treturn scores\n",
        "\n",
        "# prepare bag of words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "\t# create the tokenizer\n",
        "\ttokenizer = Tokenizer()\n",
        "\t# fit the tokenizer on the documents\n",
        "\ttokenizer.fit_on_texts(train_docs)\n",
        "\t# encode training data set\n",
        "\tx_train = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "\t# encode training data set\n",
        "\tx_test = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "\treturn x_train, x_test\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, y_train = load_clean_dataset(vocab, True)\n",
        "test_docs, y_test = load_clean_dataset(vocab, False)\n",
        "# run experiment\n",
        "modes = ['binary', 'count', 'tfidf', 'freq']\n",
        "results = DataFrame()\n",
        "for mode in modes:\n",
        "\t# prepare data for mode\n",
        "\tx_train, x_test = prepare_data(train_docs, test_docs, mode)\n",
        "\t# evaluate model on data for mode\n",
        "\tresults[mode] = evaluate_mode(x_train, y_train, x_test, y_test)\n",
        "# summarize results\n",
        "print(results.describe())\n",
        "# plot results\n",
        "results.boxplot()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "hj59nC-5LAua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.head(10)"
      ],
      "metadata": {
        "id": "wjefi7RJMFZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting Sentiment for New Reviews"
      ],
      "metadata": {
        "id": "cfuFnpNfPISo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now prepared to create and apply a final model to forecast the sentiments of new textual reviews, which is our primary objective. Initially, we'll train the final model using all the available data, opting for the 'binary' mode of the bag-of-words model, which demonstrated optimal performance in our prior assessments.\n",
        "\n",
        "The prediction process for new reviews mirrors the preparation steps for the test data. This encompasses loading the text, cleansing the document, filtering tokens based on our selected vocabulary, transforming the filtered tokens into a line, encoding them with the Tokenizer, and finally making a prediction. We can directly predict the class value using the trained model by invoking the **predict()** function, which will return 0 for a negative review and 1 for a positive one. We can consolidate these steps into a new function named **predict_sentiment()**. This function would need the review text, vocabulary, tokenizer, and trained model as inputs and would yield the predicted sentiment along with a corresponding confidence score.\n",
        "\n",
        "```python\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "    # clean\n",
        "    tokens = clean_doc(review)\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    # convert to line\n",
        "    line = ' '.join(tokens)\n",
        "    # encode\n",
        "    encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "    # predict sentiment\n",
        "    yhat = model.predict(encoded, verbose=0)\n",
        "    # retrieve predicted percentage and label\n",
        "    percent_pos = yhat[0,0]\n",
        "    if round(percent_pos) == 0:\n",
        "        return (1-percent_pos), 'NEGATIVE'\n",
        "    return percent_pos, 'POSITIVE'\n",
        "```"
      ],
      "metadata": {
        "id": "0PxII6bTRYO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "\t# load documents\n",
        "\tneg = process_docs('txt_sentoken/neg', vocab)\n",
        "\tpos = process_docs('txt_sentoken/pos', vocab)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "\t# define network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile network\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# summarize defined model\n",
        "\tmodel.summary()\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model\n",
        "\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "\t# clean\n",
        "\ttokens = clean_doc(review)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\t# convert to line\n",
        "\tline = ' '.join(tokens)\n",
        "\t# encode\n",
        "\tencoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "\t# predict sentiment\n",
        "\tyhat = model.predict(encoded, verbose=0)\n",
        "\t# retrieve predicted percentage and label\n",
        "\tpercent_pos = yhat[0,0]\n",
        "\tif round(percent_pos) == 0:\n",
        "\t\treturn (1-percent_pos), 'NEGATIVE'\n",
        "\treturn percent_pos, 'POSITIVE'\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, y_train = load_clean_dataset(vocab)\n",
        "test_docs, y_test = load_clean_dataset(vocab)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "x_train = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
        "x_test = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
        "# define network\n",
        "n_words = x_train.shape[1]\n",
        "model = define_model(n_words)\n",
        "# fit network\n",
        "model.fit(x_train, y_train, epochs=10, verbose=2)\n",
        "# test positive text\n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ],
      "metadata": {
        "id": "TRe4D-YkSYCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Exploration\n"
      ],
      "metadata": {
        "id": "YQcKT8sJUEn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This lesson provides potential enhancements to further enrich the learnings from:\n",
        "\n",
        "- **Vocabulary Managemen**: Experiment with larger or smaller vocabularies. A more optimized set of words might enhance performance.\n",
        "- **Network Topology Adjustments**: Dive into different network structures, such as deeper or broader architectures. A better-suited network could potentially improve results.\n",
        "- **Incorporate Regularization**: Examine the benefits of regularization techniques like dropout. This might help in delaying the model's convergence and enhance its performance on the test set.\n",
        "- **Enhanced Data Cleaning**: Delve deeper into the cleaning process of the review text. Adjusting the amount of cleaning might have implications on model accuracy.\n",
        "- **Training Insights**: Use the test data as a validation set during training and visualize the train-test loss through plots. Employ these insights to fine-tune parameters like batch size and training epochs.\n",
        "- **Identifying Key Words**: Investigate specific words within reviews that might strongly predict sentiments.\n",
        "- **Bigram Implementation**: Modify the model to score word bigrams and analyze its efficacy across different scoring methods.\n",
        "- **Review Truncation**: Examine the effects of truncating movie reviews on model performance. Experiment by shortening reviews from the beginning, end, or middle.\n",
        "- **Model Ensembles**: Construct models using varied word scoring methods. Check if combining these models - creating ensembles - can boost model accuracy.\n",
        "- **Evaluate Real-world Reviews**: After training a comprehensive model on all available data, assess its performance on genuine movie reviews sourced from the web."
      ],
      "metadata": {
        "id": "qCVhmy2WVGiE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CK-aYuB7VevZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}