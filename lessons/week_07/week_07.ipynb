{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Steps to Process Film Review Data for Sentiment Analysis"
      ],
      "metadata": {
        "id": "Pzw9f7PCvaNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Working with text data varies greatly depending on the specific problem at hand.\n",
        "\n",
        "While starting with foundational tasks such as loading the data might seem straightforward, the process can quickly become intricate, especially when you dive into the nuances of cleaning specific datasets. For those uncertain about where to initiate the process and the sequence of steps to transform raw data into model-ready data, this class segment offers guidance. Today, we will delve into the process of prepping movie review data for sentiment analysis. By the end of this session, students will be proficient in:\n",
        "\n",
        "- Loading text data and meticulously cleaning it to eliminate punctuation and irrelevant elements.\n",
        "- Crafting a vocabulary, refining it for relevance, and storing it for future use.\n",
        "- Processing movie reviews through meticulous cleaning and using the tailored vocabulary, subsequently saving the transformed data in formats suited for modeling.\n",
        "\n",
        "This section is segmented into the subsequent sections:\n",
        "\n",
        "1. Overview of the Movie Review Dataset\n",
        "2. Importing Text Data\n",
        "3. Text Data Cleansing\n",
        "4. Vocabulary Creation\n",
        "5. Storing the Processed Data."
      ],
      "metadata": {
        "id": "AUrwKeNUvnOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Movie Review Dataset"
      ],
      "metadata": {
        "id": "ZeH2MuOLwcq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **``Movie Review Data``** is an aggregated dataset procured from the [IMDb](https://www.imdb.com/) domain during the early 2000s, diligently compiled by researchers Bo Pang and Lillian Lee.\n",
        "\n",
        "These critiques were extracted and subsequently made public as a pivotal component of their exploration into natural language processing (NLP). Initially launched in 2002, the dataset experienced iterative enhancements and was subsequently versioned as 2.0 in 2004.\n",
        "\n",
        "> It encapsulates 1,000 positive and 1,000 negative film evaluations, mined from the rec.arts.movies.reviews newsgroup hosted on IMDb's infrastructure.\n",
        "\n",
        "Within the realm of computational linguistics and data analytics, Pang and Lee consistently refer to this collection as the **``polarity dataset.``**"
      ],
      "metadata": {
        "id": "KLVow-ZmGAr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=10OsDrN-m2IIKqZJrf-xMbEtg8VlGJ2DQ"
      ],
      "metadata": {
        "id": "2b3PQeJZGmyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've decompressed the file, you'll find a folder named **``txt_sentoken``**. Inside this folder, there are two subfolders: **``neg``** and **``pos``**, which hold the negative and positive reviews, respectively. Each review is saved in its individual file, following a naming pattern ranging from **``cv000``** to **``cv999``** for both neg and pos categories. We'll now proceed to examine how to load this text data."
      ],
      "metadata": {
        "id": "KIZRJqoHHSUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf review_polarity.tar.gz"
      ],
      "metadata": {
        "id": "5ZG0O44eIWU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load"
      ],
      "metadata": {
        "id": "h2tzSD9WIaw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll delve into loading individual text files and subsequently iterating over directories of files. It's expected that the review dataset is locally stored in the **``txt sentoken``** directory of the current workspace. Standard file I/O operations involve opening a file, reading its ASCII content, and terminating the session. For demonstration purposes, to read the initial negative review file named **``cv000_29416.txt``**."
      ],
      "metadata": {
        "id": "jE6Mc07mjwFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load the document\n",
        "filename = 'txt_sentoken/neg/cv000_29416.txt'\n",
        "text = load_doc(filename)\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "id": "Hy-ijpYQYFYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This reads the document in ASCII format, retaining any white spaces, including line breaks. We can encapsulate this process into a function named **``load_doc()``**, which accepts a filename as its argument and outputs the corresponding text."
      ],
      "metadata": {
        "id": "9BrfKasIjBnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Text Data"
      ],
      "metadata": {
        "id": "ZlkngjlskJ0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll load a document and examine the raw tokens divided by white space. We'll utilize the **``load_doc()``** function from the earlier section. The **``split()``** method can be employed to segment the loaded document into tokens based on white space."
      ],
      "metadata": {
        "id": "d-uTvrgrk5OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into tokens by white space\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "n_OEtt4Nkr1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive',\n",
        "'.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend',\n",
        "'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', \"what's\", 'the', 'deal',\n",
        " '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind-fuck',\n",
        "'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but',\n",
        "'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review',\n",
        "'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt',\n",
        "'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and','such', '(', 'lost', 'highway', '&',\n",
        "'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films',\n",
        "',', 'and', 'these', 'folks', 'just', \"didn't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to',\n",
        "'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are',\n",
        "'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', \"it's\", 'simply',\n",
        "'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience',\n",
        "'member', ',', 'have', 'no', 'idea', \"what's\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are',\n",
        "'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the',\n",
        "'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a',\n",
        "'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and',\n",
        "'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', \"don't\", 'mind', 'trying', 'to',\n",
        "'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me',\n",
        "'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',',\n",
        "'which', 'is', 'this', \"film's\", 'biggest', 'problem', '.', \"it's\", 'obviously', 'got', 'this', 'big', 'secret',\n",
        " 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final',\n",
        " 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging',\n",
        "',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i',\n",
        "'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by',\n",
        "'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make',\n",
        "'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', \"didn't\", 'the', 'make', 'the', 'film', 'all', 'that',\n",
        "'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you',\n",
        "'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they',\n",
        "'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',',\n",
        "'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout',\n",
        "'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', \"don't\", 'know', 'who', 'they', 'are',\n",
        " '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving',\n",
        " 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down',\n",
        "'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director',\n",
        "'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', \"might've\", 'been', 'a', 'pretty',\n",
        "'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided',\n",
        "'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense',\n",
        "'.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed',\n",
        "'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only',\n",
        "'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds',\n",
        "'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', \"character's\",\n",
        "'unraveling', '.', 'overall', ',', 'the', 'film', \"doesn't\", 'stick', 'because', 'it', \"doesn't\", 'entertain', ',',\n",
        "\"it's\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of',\n",
        "'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the',\n",
        "'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not',\n",
        "'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', \"it's\", 'just', 'packaged', 'to', 'look', 'that', 'way',\n",
        "'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids',\n",
        "'.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves',\n",
        "'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', \"where's\", 'joblo', 'coming', 'from', '?', 'a', 'nightmare',\n",
        "'of', 'elm', 'street', '3', '(', '7/10', ')', '-', 'blair', 'witch', '2', '(', '7/10', ')', '-', 'the', 'crow', '(', '9/10',\n",
        "')', '-', 'the', 'crow', ':', 'salvation', '(', '4/10', ')', '-', 'lost', 'highway', '(', '10/10', ')', '-', 'memento',\n",
        " '(', '10/10', ')', '-', 'the', 'others', '(', '9/10', ')', '-', 'stir', 'of', 'echoes', '(', '8/10', ')']\n",
        "```"
      ],
      "metadata": {
        "id": "LDbEcERKltrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examining the raw tokens provides numerous insights into potential preprocessing steps, including:\n",
        "\n",
        "- Stripping words of punctuation (for instance, **``what's``**).\n",
        "- Discarding tokens composed solely of punctuation (like **``-``**).\n",
        "- Eliminating tokens with numbers (such as **``10/10``**).\n",
        "- Omitting single-character tokens (like **``a``**).\n",
        "- Excluding tokens of little semantic value (like **``and``**).\n",
        "\n",
        "Here are some strategies:\n",
        "\n",
        "- To filter out punctuation from tokens, we can use **``regular expressions``**.\n",
        "- Tokens that are solely punctuation or contain numbers can be removed with an **``isalpha()``** check.\n",
        "- Common English **``stop words``** can be eliminated using a list from **``NLTK``**.\n",
        "- Short tokens can be filtered by assessing their length.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W4yiYWS_k-2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "MUcSJ5GI0JXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load the document\n",
        "filename = 'txt_sentoken/neg/cv000_29416.txt'\n",
        "text = load_doc(filename)\n",
        "\n",
        "# split into tokens by white space\n",
        "tokens = text.split()\n",
        "\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "LcihXd66lrwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can encapsulate this in a function named **``clean_doc()``** and try it on a different review, preferably a positive one."
      ],
      "metadata": {
        "id": "CZlDvphcz925"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load the document\n",
        "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "VZNer9fJ1mAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cleaning process appears to yield a satisfactory collection of tokens as an initial attempt. There are additional cleaning measures we could consider, and I'll leave those to your creativity. Now, let's explore how we can curate a desired set of tokens."
      ],
      "metadata": {
        "id": "wzy2IVJP11-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct a Vocabulary"
      ],
      "metadata": {
        "id": "dFIsqjbO2Kpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When creating predictive text models, such as the **``bag-of-words``** model, there's a need to minimize the vocabulary size. A more extensive vocabulary leads to sparser representations of each word or document. Part of the text preparation for sentiment analysis is determining and customizing the word vocabulary that the model will recognize. This can be accomplished by loading all documents in the dataset and creating a word set. We might choose to include all these words or possibly exclude some. The finalized vocabulary can be saved for future use, such as when filtering words in upcoming documents.\n",
        "\n",
        "A useful tool for this task is the **``Counter``**, which acts as a dictionary cataloging words and their frequencies, equipped with some handy additional functions. We should design a function that processes a document and integrates it into the vocabulary. This function should:\n",
        "\n",
        "1. Load a document using the previously defined **``load_doc()``** function.\n",
        "2. Clean the document utilizing the **``clean_doc()``** function.\n",
        "3. Introduce all the tokens to the **``Counter``** and refresh their counts. The **``update()``** function on the counter object can achieve this.\n",
        "\n",
        "Here's a function named **``add_doc_to_vocab()``** that accepts a document filename and a **``Counter``** vocabulary as its parameters."
      ],
      "metadata": {
        "id": "xI48EwVX2bDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\n",
        "\t\t# skip files that do not have the right extension\n",
        "\t\tif not filename.endswith(\".txt\"):\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "metadata": {
        "id": "EZrsIZK73Kvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(vocab)"
      ],
      "metadata": {
        "id": "829pGBVz30VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lenght of vocabulary\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "F4tigd-768X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the example generates a vocabulary that encompasses all documents in the dataset, spanning both positive and negative reviews. The data reveals that there are slightly more than 46,000 distinct words throughout all the reviews. The three most prevalent words are \"film,\" \"one,\" and \"movie.\""
      ],
      "metadata": {
        "id": "K5UNAjVz4JN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> Words that are extremely rare, appearing only once in all reviews, may not be indicative. Similarly, some frequently occurring words might also lack predictive value.\n",
        "\n",
        "It's essential to validate these assumptions using a specific predictive model. Typically, words that show up once or just a handful of times in 2,000 reviews might not offer any predictive power and can be excluded from our vocabulary. This significantly reduces the number of tokens to be modeled. To achieve this, we can sift through the words and their frequencies, retaining only those that surpass a set threshold. In this context, **we'll consider words that appear more than five times**."
      ],
      "metadata": {
        "id": "QrTS7QO04o9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# keep tokens with > 5 occurrence\n",
        "min_occurrence = 5\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
        "print(len(tokens))\n",
        "```"
      ],
      "metadata": {
        "id": "yzhAS3ij4rfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary shrinks drastically from **``46,557 to 14,803``** words with this approach.\n",
        "\n",
        "> Setting a minimum threshold of 5 occurrences might be too stringent; feel free to try different thresholds.\n",
        "\n",
        "After finalizing the vocabulary, it can be saved to a new file. I prefer saving the vocabulary in ASCII format, with each word on a separate line. The following introduces a function named **``save_list()``** which saves a list of items - in this instance, tokens - to a file, with each token on a distinct line."
      ],
      "metadata": {
        "id": "GCiBY9V_7iRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip files that do not have the right extension\n",
        "\t\tif not filename.endswith(\".txt\"):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# keep tokens with > 5 occurrence\n",
        "min_occurrence = 5\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
        "print(len(tokens))\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "metadata": {
        "id": "xopLHzuE9GV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Prepared Data"
      ],
      "metadata": {
        "id": "-H7xYI4bZFNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can employ the cleaned data and selected vocabulary to process each film review and then store these processed reviews, ensuring they're set for modeling. Separating data preparation from modeling is advisable, as it lets you concentrate on the modeling phase and revisit data preparation if innovative concepts arise. Initially, we can load the vocabulary from the file **``vocab.txt``**."
      ],
      "metadata": {
        "id": "RelHDRfZZCa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)"
      ],
      "metadata": {
        "id": "PH0ITrgk95vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can process the reviews, utilize the loaded vocabulary to filter out undesired tokens, and store the refined reviews in a new file. One strategy might be to store all positive reviews in one file and all negative reviews in another, with the filtered tokens distinguished by spaces, and each review on distinct lines. To begin, we should formulate a function that processes a document, refines it, filters it, and returns it as a single line for storage. The function **``doc_to_line()``** detailed below accomplishes this, accepting a filename and vocabulary (in set form) as parameters. This function leverages the previously established **``load_doc()``** to retrieve the document and **``clean_doc()``** to tokenize it.\n",
        "\n",
        "\n",
        "```python\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "    # load the doc\n",
        "    doc = load_doc(filename)\n",
        "    \n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    \n",
        "    return ' '.join(tokens)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1PUUe1ff_VWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can introduce a revised version of **``process_docs()``** that will iterate through all the reviews in a directory and transform them into lines using the **``doc_to_line()``** function for each file. This will result in a list of lines being produced.\n",
        "\n",
        "```python\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "    lines = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "        # skip files that do not have the right extension\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load and clean the doc\n",
        "        line = doc_to_line(path, vocab)\n",
        "        # add to list\n",
        "        lines.append(line)\n",
        "    return lines\n",
        "```"
      ],
      "metadata": {
        "id": "EMOi9ctbaGSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can subsequently invoke **``process_docs()``** for both the directories containing positive and negative reviews. After that, we can use the **``save_list()``** function, mentioned earlier, to store each list of refined reviews in a file.\n"
      ],
      "metadata": {
        "id": "E25gm3vqakVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "\t# load the doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# filter by vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\treturn ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip files that do not have the right extension\n",
        "\t\tif not filename.endswith(\".txt\"):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tline = doc_to_line(path, vocab)\n",
        "\t\t# add to list\n",
        "\t\tlines.append(line)\n",
        "\treturn lines\n",
        "\n",
        "# load vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "# prepare negative reviews\n",
        "negative_lines = process_docs('txt_sentoken/neg', vocab)\n",
        "save_list(negative_lines, 'negative.txt')\n",
        "# prepare positive reviews\n",
        "positive_lines = process_docs('txt_sentoken/pos', vocab)\n",
        "save_list(positive_lines, 'positive.txt')"
      ],
      "metadata": {
        "id": "WWvpQLi0OCjp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}